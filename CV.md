## 3D Model Reconstruction
Images with a single camera can generate 3d model.
https://kotai2003-faces.streamlit.app/

## Face Pyramid Vision Transformer
- 33rd British Machine Vision Conference (BMVC) 2022, 21st - 24th November 2022, London, UK. 
- Project Page: https://khawar-islam.github.io/fpvt/
- Code: https: https://github.com/khawar-islam/FPVT_BMVC22

#### Highlights:
A novel Face Pyramid Vision Transformer (FPVT) is proposed to learn a discriminative multi-scale facial representations for face recognition and verification. In FPVT, Face Spatial Reduction Attention (FSRA) and Dimensionality Reduction (FDR) layers are employed to make the feature maps compact, thus reducing the computations. An Improved Patch Embedding (IPE) algorithm is proposed to exploit the benefits of CNNs in ViTs (e.g., shared weights, local context, and receptive fields) to model lower-level edges to higher-level semantic primitives. Within FPVT framework, a Convolutional Feed-Forward Network (CFFN) is proposed that extracts locality information to learn low level facial information. The proposed FPVT is evaluated on seven benchmark datasets and compared with ten existing state-of-the-art methods, including CNNs, pure ViTs, and Convolutional ViTs. Despite fewer parameters, FPVT has demonstrated excellent performance over the compared methods. I am greatly thankful to my coauthors Arif Mahmood and Zaigham Zaheer for their supervision and guidance throughout the project.

## DeepMind Introduces the Perception Test, a New Multimodal Benchmark Using Real-World Videos to Help Evaluate the Perception Capabilities of a Machine Learning Model
Paper: https://storage.googleapis.com/.../perception_test_report...
Github link: https://github.com/deepmind/perception_test

## Latest Computer Vision Research Proposes Lumos for Relighting Portrait Images via a Virtual Light Stage and Synthetic-to-Real Adaptation
Paper: https://arxiv.org/pdf/2209.10510.pdf
Demo: http://imaginaire.cc/Lumos/
Project: https://deepimagination.cc/Lumos/

## Google AI Introduces Frame Interpolation for Large Motion (FILM): A New Neural Network Architecture To Create High-Quality Slow-Motion Videos From Near-Duplicate Photos
Paper: https://arxiv.org/pdf/2202.04901.pdf
Github: https://github.com/google-research/frame-interpolation
Project: https://film-net.github.io/

## Latest Computer Vision Research at Nanyang Technological University Introduces VToonify Framework for Style Controllable High-Resolution Video Toonification
Paper: https://arxiv.org/pdf/2209.11224v2.pdf
Github: https://github.com/williamyang1991/vtoonify

## Salesforce AI Open-Sources ‘LAVIS,’ A Deep Learning Library For Language-Vision Research/Applications
Paper: https://arxiv.org/pdf/2209.09019.pdf
Github link: https://github.com/salesforce/LAVIS

## Researchers at Tencent Propose GFP-GAN that Leverages Rich and Diverse Priors Encapsulated in a Pretrained Face GAN for Blind Face Restoration
Paper: https://arxiv.org/pdf/2101.04061v2.pdf
Github link: https://github.com/TencentARC/GFPGAN

## Latest Computer Vision Research at Google and Boston University Proposes ‘DreamBooth,’ 
A Technique for Fine-Tuning a Text-to-Image Model with a very Limited Set of Images
Paper: https://arxiv.org/pdf/2208.12242.pdf?
Project: https://dreambooth.github.io/

## Researchers from McGill University and Microsoft Introduces Convolutional vision Transformer (CvT) that improves Vision Transformer (ViT) in Performance and Efficiency by Introducing Convolutions into ViT
Paper: https://openaccess.thecvf.com/.../Wu_CvT_Introducing...
GIthub: https://github.com/microsoft/CvT

## Microsoft Research Introduces a General-Purpose Multimodal Foundation Model ‘BEIT-3,’ that Achieves State-of-the-Art Transfer Performance on Both Vision and Vision Language Tasks
Paper: https://arxiv.org/pdf/2208.10442.pdf
Github: https://github.com/microsoft/unilm/tree/master/beit

## Apple Researchers Develop NeuMan
A Novel Computer Vision Framework that can Generate Neural Human Radiance Field from a Single Video
Paper: https://arxiv.org/pdf/2203.12575v1.pdf
Github: https://github.com/apple/ml-neuman

## Researchers from the Alibaba Group added their newly developed ‘YOLOX-PAI’ into EasyCV, which is an all-in-one Computer Vision Toolbox
Paper: https://arxiv.org/pdf/2208.13040v1.pdf
Github link: https://github.com/alibaba/EasyCV

## Deepmind Researchers Introduce ‘Transframer’: 
A General-Purpose AI Framework For Image Modelling And Computer Vision Tasks Based On Probabilistic Frame Prediction
Paper: https://arxiv.org/pdf/2203.09494.pdf

## Researchers at Apple Develop Texturify: A GAN-based Approach for Generating Textures on 3D Shape Surfaces
Paper: https://nihalsid.github.io/texturify/static/Texturify.pdf
Project: https://nihalsid.github.io/texturify/

## Latest Computer Vision Research At Microsoft Explains How This Proposed Method Adapts The Pretrained Language Image Models To Video Recognition
Paper: https://arxiv.org/pdf/2208.02816v1.pdf
Github: https://github.com/microsoft/VideoX/tree/master/X-CLIP

## Salesforce AI Propose A Novel Framework That Trains An Open Vocabulary Object Detector With Pseudo Bounding-Box Labels Generated From Large-Scale Image-Caption Pairs
Paper: https://arxiv.org/pdf/2111.09452.pdf
GItHub link: https://arxiv.org/pdf/2111.09452.pdf

## Researchers Present A Survey Report on Using 100+ Transformer-based Methods in Computer Vision for Different 3D Vision Tasks
Paper:  https://arxiv.org/pdf/2208.04309v1.pdf
Github link: https://github.com/lahoud/3d-vision-transformers

## Researchers Propose Easter2.0, a Novel Convolutional Neural Network CNN-Based Architecture for the Task of End-to-End Handwritten Text Line Recognition that Utilizes Only 1D Convolutions
Paper: https://arxiv.org/pdf/2205.14879v1.pdf
Github link: https://github.com/kartikgill/easter2

## Researchers at Meta AI Develop Multiface: A Dataset for Neural Face Rendering
Paper: https://arxiv.org/pdf/2207.11243v1.pdf
Github link: https://github.com/facebookresearch/multiface

## Research From China Propose a Novel Context-Aware Vision Transformer (CA-ViT) For Ghost-Free High Dynamic Range Imaging
They propose a novel vision transformer termed CA-ViT that can fully utilize both global and local picture context dependencies while outperforming its predecessors by a wide margin.
They introduce a unique HDR-Transformer that can reduce processing costs, ghosting artifacts, and recreating high-quality HDR photos. This is the first Transformer-based HDR de-ghosting framework to be developed. 
They undertake in-depth tests on three sample benchmark HDR datasets to compare HDR-performance Transformers to current state-of-the-art techniques.
Paper: https://arxiv.org/pdf/2208.05114v1.pdf
Github link: https://github.com/megvii-research/HDR-Transformer

## DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation (CVPR 2022) 
Abstract: Recently, GAN inversion methods combined with Contrastive Language-Image Pretraining (CLIP) enables zero-shot image manipulation guided by text prompts. However, their applications to diverse real images are still difficult due to the limited GAN inversion capability. Specifically, these approaches often have difficulties in reconstructing images with novel poses, views, and highly variable contents compared to the training data, altering object identity, or producing unwanted image artifacts. To mitigate these problems and enable faithful manipulation of real images, we propose a novel method, dubbed DiffusionCLIP, that performs text-driven image manipulation using diffusion models. Based on full inversion capability and high-quality image generation power of recent diffusion models, our method performs zero-shot image manipulation successfully even between unseen domains and takes another step towards general application by manipulating images from a widely varying ImageNet dataset. Furthermore, we propose a novel noise combination method that allows straightforward multi-attribute manipulation. Extensive experiments and human evaluation confirmed robust and superior manipulation performance of our methods compared to the existing baselines. 

Source: https://openaccess.thecvf.com/.../Kim_DiffusionCLIP_Text...
Slides: https://www.slideshare.net/.../diffusionclip-textguided...
Video: https://youtu.be/YVCtaXw6fw8
Code: https://github.com/gwang-kim/DiffusionCLIP.git

## NVIDIA AI Researchers Propose ‘MinVIS,’ 
A Minimal Video Instance Segmentation (VIS) Framework That Achieves SOTA Performance With Neither Video-Based Architectures Nor Training Procedures
Paper: https://arxiv.org/pdf/2208.02245v1.pdf
Github link: https://github.com/nvlabs/minvis

## Researchers from China Propose DAT: a Deformable Vision Transformer to Compute Self-Attention in a Data-Aware Fashion
Paper: https://openaccess.thecvf.com/.../Xia_Vision_Transformer...
Github: https://github.com/LeapLabTHU/DAT

## Researchers From CMU And Stanford Develop OBJECTFOLDER 2.0: A Multisensory Object Dataset For Sim2Real Transfer
Paper: https://arxiv.org/pdf/2204.02389.pdf
Github: https://github.com/rhgao/ObjectFolder
Project: https://ai.stanford.edu/~rhgao/objectfolder2.0/

## image classification on small-datasets in Pytorch
https://github.com/Harry-KIT/Image-Classification-on-small-datasets-in-Pytorch

## Alibaba AI Research Team Introduces ‘DCT-Net’
A Novel Image Translation Architecture For Few-Shot Portrait Stylization
Paper: https://arxiv.org/pdf/2207.02426v1.pdf
Project: https://menyifang.github.io/projects/DCTNet/DCTNet.html
Github link: https://github.com/menyifang/dct-net

## NeurIPS2021 spotlight work PCAN-“Prototypical Cross-Attention Networks for Multiple Object Tracking and Segmentation”.
- PCAN uses test-time prototypes to memorize instance appearance and achieve impressive seg tracking accuracy on YT-VIS and BDD100K.
- Code: https://github.com/SysCV/pcan
- Paper: https://arxiv.org/abs/2106.11958

## yolo v7
YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors
공헌자(저자): Chien-Yao Wang, Alexey Bochkovskiy, Hong-Yuan Mark Liao
논문: https://arxiv.org/abs/2207.02696
GitHub: https://github.com/wongkinyiu/yolov7

초록: YOLOv7은 5FPS~160FPS 범위에서 속도와 정확도 모두에서 알려진 모든 객체 감지기를 능가하며 GPU V100에서 30FPS 이상의 알려진 모든 실시간 객체 감지기 중 가장 높은 정확도 56.8% AP를 가지고 있습니다. YOLOv7-E6 물체 감지기(56 FPS V100, 55.9% AP)는 변압기 기반 감지기인 SWIN-L Cascade-Mask R-CNN(9.2 FPS A100, 53.9% AP)보다 속도 509%, 정확도 2%, 컨볼루션 기반 검출기 ConvNeXt-XL Cascade-Mask R-CNN(8.6 FPS A100, 55.2% AP)은 속도 551%, AP 정확도 0.7% 향상 및 YOLOv7 성능 향상: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B 및 기타 여러 물체 감지기의 속도와 정확도. 또한 다른 데이터 세트나 사전 훈련된 가중치를 사용하지 않고 처음부터 MS COCO 데이터 세트에서만 YOLOv7을 훈련합니다.

## Diffusion Model
Diffusion 입문
https://lilianweng.github.io/.../2021-07-11-diffusion.../
https://youtu.be/d_x92vpIWFM
================================
Diffusion Models Beat GANs on Image Synthesis
확산 모델에 classifier를 추가해 다양성-품질 trade-off를 달성
Classifier-Free Diffusion Guidance
Classifier 없이 단일 확산 모델로 같은 목표 달성
Cascaded Diffusion Models for High Fidelity Image Generation
여러 해상도의 확산 모델이 포함된 계층적 cascading pipeline으로 이전보다 더 높은 해상도에서 고품질 샘플 생성
Pretraining is All You Need for Image-to-Image Translation (PITI)
사전 훈련된 확산 모델을 이용해 다양한 다운스트림 작업 입력 조건(e.g. semantic map + text)에서 Image-to-Image translation 수행

## akka-stream
다이나믹 배치를 구현하고 있고 충분히 작은 모델이라면 10000 ~ 20000  requests / sec 수준의 응답 성능을 낼 수 있습니다.
구현에 사용된 akka는 프레임워크가 아닌 고도의 동시성, 병렬성, 분산성을 가지고 있는 메세지 기반 어플레케이션 구축 툴킷으로 간주할 수 있습니다.
충분히 생산성이 있는 언어를 베이스로 하고 있기 때문에 직접 서빙 데몬에서 monolithic 한 구조로 비지니스 코드를 내재화 하는것도 가능합니다. 
예제의 코드량이 적고 추상화가 거의 없는 naive한 구현이기 때문에 동작에 관련한 거의 대부분의 요소를 블랙박스 없이 확인하고 동시에 환경 튜닝이 가능합니다.
실제 예제의 사용성은 웹과 상호 작용을 하는 어플리케이션 보다는 검색, 추천, 대화 시스템등 다수의 모델을 컨트롤하는 기반 플랫폼 시스템에 적합합니다.
https://github.com/go-noah/akka-dynamic-batch-serving/tree/main/akka-dynamic-batch-onnx-gpu-bert
https://github.com/go-noah/akka-dynamic-batch-serving/tree/main/akka-dynamic-batch-tensorflow-gpu-bert

## AI2’s PRIOR Team Introduces Unified-IO:
The First Neural Model To Execute Various AI Tasks Spanning Classical Computer Vision, Image Synthesis, Vision-and-Language, and Natural Language Processing NLP
Demo: https://unified-io.allenai.org/

## AI Researchers From China Introduce a New Vision GNN (ViG) Architecture to Extract Graph Level Feature for Visual Tasks
Paper: https://arxiv.org/pdf/2206.00272v1.pdf
Github: https://github.com/huawei-noah/Efficient-AI-Backbones

## Researchers at Stanford have developed an Artificial Intelligence (AI) model,
EG3D, that can generate random images of faces and other objects with high resolution together with underlying geometric structures
[Quick Read: https://www.marktechpost.com/2022/07/04/researchers-at-stanford-have-developed-an-artificial-intelligence-ai-model-eg3d-that-can-generate-random-images-of-faces-and-other-objects-with-high-resolution-together-with-underlying-geometric-s/?fbclid=IwAR3s59QXgJsrYG0uIiDTIIQl784LAUe48NrfJ6Vk6kTVVOjjHAzod7DRAEc
Paper: https://openaccess.thecvf.com/content/CVPR2022/papers/Chan_Efficient_Geometry-Aware_3D_Generative_Adversarial_Networks_CVPR_2022_paper.pdf?fbclid=IwAR2oL0AvGr_0uBamWB67pHl_KNSAuhxN2VKpyzLcpGiIBVIyJiy7211j_8M
Github: https://github.com/NVlabs/eg3d

## Stanford and TRI AI Researchers Propose the Atemporal Probe (ATP), A New ML Model For Video-Language Analysis
Paper: https://arxiv.org/pdf/2206.01720.pdf
Project: https://stanfordvl.github.io/atp-revisit-video-lang/

## A New Technique to Train Diffusion Model in Latent Space Using Limited Computational Resources While Maintaining High-Resolution Quality
Paper: https://arxiv.org/pdf/2112.10752.pdf
Github: https://github.com/CompVis/latent-diffusion

## MPViT
Arxiv https://arxiv.org/abs/2112.11010
Code https://github.com/youngwanLEE/MPViT

## DN-DETR: Accelerate DETR Training by Introducing Query DeNoising
## DAB-DETR : Dynamic Anchor Boxes are Better Queries for DETR 

## Dynamic Gender Classification
code : https://github.com/CChenLi/Dynamic_Gender_Classification

## NVIDIA의 Efficient Geometry-aware 3D Generative Adversarial Networks(EG3D)
code: https://github.com/NVlabs/eg3d
paper: https://arxiv.org/abs/2112.07945
page: https://nvlabs.github.io/eg3d/
youtube: https://youtu.be/cXxEwI7QbKg

## Salesforce AI Research has proposed a new video-and-language representation learning framework called ALPRO. 
This framework can be used for pre-training models to achieve state-of-the-art performance on tasks such as video-text retrieval and question answering.
Paper: https://arxiv.org/pdf/2112.09583.pdf
Github: https://github.com/salesforce/alpro

## Warehouse Apparel Detection using YOLOv5 end to end project
Kindly Like and Share and subscribe to the YT channel !!
Project Code: https://github.com/Ashishkumar-hub/Warehouse-Apparel-Detection-using...

## Researchers From MIT and Cornell Develop STEGO 
(Self-Supervised Transformer With Energy-Based Graph Optimization): A Novel AI Framework That Distills Unsupervised Features Into High-Quality Discrete Semantic Labels
Paper: https://arxiv.org/pdf/2203.08414.pdf
Github: https://github.com/mhamilton723/STEGO

## UTokyo Researchers Introduce 
A Novel Synthetic Training Data Called Self-Blended Images (SBIs) To Detect Deepfakes
Paper: https://arxiv.org/pdf/2204.08376.pdf
Github: https://github.com/mapooon/SelfBlendedImages

## Meta AI Introduces ‘Make-A-Scene’: 
A Deep Generative Technique Based On An Autoregressive Transformer For Text-To-Image Synthesis With Human Priors
Paper Summary: https://www.marktechpost.com/.../meta-ai-introduces-make.../
Paper: https://arxiv.org/pdf/2203.13131v1.pdf

## Bytedance Researchers Propose CLIP-GEN: 
A New Self-Supervised Deep Learning Generative Approach Based On CLIP And VQ-GAN To Generate Reliable Samples From Text Prompts
Paper: https://arxiv.org/pdf/2203.00386v1.pdf

## Warehouse Apparel Detection using YOLOv5 end to end project
Kindly Like and Share and subscribe to the YT channel !!
Project Code: https://github.com/.../Warehouse-Apparel-Detection-using...

## Learning to Estimate Robust 3D Human Mesh from In-the-Wild Crowded Scenes / 3DCrowdNet
https://arxiv.org/abs/2104.07300
github: https://github.com/hongsukchoi/3DCrowdNet_RELEASE

## Google AI Researchers Propose SAVi++: 
An Object-Centric Video Model Trained To Predict Depth Signals From A Slot-Based Video Representation
Paper: https://arxiv.org/pdf/2206.07764.pdf
Project: https://slot-attention-video.github.io/savi++/

## Meta AI Research Proposes ‘OMNIVORE’: 
A Single Vision (Computer Vision) Model For Many Different Visual Modalities
Paper: https://arxiv.org/abs/2201.08377
Github: https://github.com/facebookresearch/omnivore

## 젯슨나노를 이용해서 녹색 이구아나의 외래 종의 실시간 탐지 및 모니터링
GitHub: https://github.com/.../Iguana-detection-on-Nvidia-Jetson...
블로그 링크: https://blogs.nvidia.com.tw/.../green-iguana-detection.../
