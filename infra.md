1. 비하인드 스토리: DeepSeek Open Source Week의 배경과 의도
배경: 2025년 1월, 중국의 AI 스타트업 DeepSeek은 DeepSeek-R1이라는 대형 추론 특화 언어 모델을 공개하며 AI 업계를 놀라게 했습니다. DeepSeek-R1은 MIT 라이선스로 공개되었으며 성능이 OpenAI GPT-4에 필적한다고 알려졌습니다 (DeepSeek — Wikipedia). 특히 R1의 훈련 비용은 경쟁 모델들보다 훨씬 저렴하여(약 600만 달러, GPT-4의 1/16 수준) 업계에 큰 충격을 주었고, AI 하드웨어 및 클라우드 기업들의 주가에 영향을 줄 정도였습니다 (DeepSeek — Wikipedia). 이러한 성공은 “AI 판도를 뒤흔들었다”고 평가되었으며, 중국처럼 최신 GPU 공급이 제한된 환경에서 효율성 혁신으로 돌파구를 마련한 사례로 주목받았습니다 (DeepSeek — Wikipedia).
의도: DeepSeek 팀은 이러한 성과를 더 폭넓은 협업과 혁신으로 이어가기 위해 Open Source Week를 기획했습니다. 2025년 2월 24일부터 5일간 진행된 이 행사에서, DeepSeek은 자사의 핵심 AI 인프라 기술 5종을 하루에 하나씩 오픈소스로 공개했습니다 (GitHub — deepseek-ai/open-infra-index: Production-tested AI infrastructure tools for efficient AGI development and community-driven innovation). 이 결정에는 AI 개발의 투명성, 커뮤니티 기여, 그리고 연구개발 가속화에 대한 DeepSeek의 철학이 담겨 있습니다. DeepSeek는 “우리는 거창한 주장을 내세우는 대신, 작은 진전이라도 완전한 투명성으로 공유한다”면서, “공유되는 코드 한 줄 한 줄이 모여 집단적 추진력이 된다”고 강조했습니다 (GitHub — deepseek-ai/open-infra-index: Production-tested AI infrastructure tools for efficient AGI development and community-driven innovation). 또한 “상아탑을 벗어나 차고(garage) 정신과 커뮤니티 주도 혁신으로 함께 나아가자”고 언급하여, 거대 기업 중심의 폐쇄적 개발이 아닌 개방형 협업 생태계를 지향함을 분명히 했습니다 (GitHub — deepseek-ai/open-infra-index: Production-tested AI infrastructure tools for efficient AGI development and community-driven innovation).
이 시기는 Meta와 xAI 등 다른 기업들도 일부 기술을 공개하며 AI 오픈소스에 대한 수요가 상승하던 때로 (Deepseek Open Source Week Kicked off with FlashMLA(Github Codebase Included!) — DEV Community), DeepSeek는 이러한 흐름에 발맞춰 자신들의 생산 수준 코드와 노하우를 적극 공유함으로써 업계에 투명성과 협력의 중요성을 환기하고자 했습니다. 특히 DeepSeek는 R1 모델 공개 이후 제기된 비판(예: 모델이 민감 주제에 응답하지 않는 검열 이슈 (DeepSeek promises to open-source 5 new repos starting next week | Best course on large language models free | Chatgpt llm parameters | Large language models course free online | Turtles AI) (DeepSeek promises to open-source 5 new repos starting next week | Best course on large language models free | Chatgpt llm parameters | Large language models course free online | Turtles AI))에도 대응하여, 오픈소싱을 통해 더 광범위한 평가와 개선을 받겠다는 의지를 보였습니다 (DeepSeek promises to open-source 5 new repos starting next week | Best course on large language models free | Chatgpt llm parameters | Large language models course free online | Turtles AI) (DeepSeek promises to open-source 5 new repos starting next week | Best course on large language models free | Chatgpt llm parameters | Large language models course free online | Turtles AI). 요컨대 Open Source Week는 DeepSeek의 철학적 선언이자, “AGI 개발을 모두와 함께 가속”하겠다는 실천으로 기획된 것입니다.
￼
DeepSeek Open Source Week 동안 매일 공개된 주요 기술과 내용을 정리하면 다음과 같습니다 (GitHub — deepseek-ai/open-infra-index: Production-tested AI infrastructure tools for efficient AGI development and community-driven innovation) (GitHub — deepseek-ai/open-infra-index: Production-tested AI infrastructure tools for efficient AGI development and community-driven innovation):
￼
* 		 FlashMLA: “Efficient MLA Decoding Kernel” — Hopper GPU용 고성능 디코딩 커널. 가변 길이 시퀀스 처리를 최적화하여 대용량 언어모델의 디코딩 속도를 극대화하는 라이브러리입니다 (GitHub — deepseek-ai/open-infra-index: Production-tested AI infrastructure tools for efficient AGI development and community-driven innovation). BF16(Brain Floating Point 16) 지원, 페이징 기법의 KV 캐시(블록 크기 64) 등을 갖추고 있으며, H800 GPU에서 최대 3000GB/s 메모리 대역폭, 580TFLOPS 연산처리를 달성했습니다 (GitHub — deepseek-ai/open-infra-index: Production-tested AI infrastructure tools for efficient AGI development and community-driven innovation). Multi-Head Latent Attention(MLA)라는 DeepSeek 고유의 어텐션 최적화를 구현하여, 불규칙한 길이의 입력도 효율적으로 처리합니다 (Deepseek Open Source Week Kicked off with FlashMLA(Github Codebase Included!) — DEV Community) (Deepseek Open Source Week Kicked off with FlashMLA(Github Codebase Included!) — DEV Community).

* 		DeepGEMM: “FP8 GEMM Library with Fine-Grained Scaling” — FP8(8비트 부동소수) 행렬연산(GEMM) 라이브러리입니다. Dense (밀집) 연산과 MoE용 그룹화 연산 모두를 지원하며, Hopper Tensor Core의 FP8 연산을 최대 활용합니다 (GitHub — deepseek-ai/open-infra-index: Production-tested AI infrastructure tools for efficient AGI development and community-driven innovation) (GitHub — deepseek-ai/open-infra-index: Production-tested AI infrastructure tools for efficient AGI development and community-driven innovation). 약 300줄의 핵심코드로 구성된 경량 설계이지만, 전문가가 튜닝한 라이브러리보다도 우수한 성능을 대부분의 매트릭스 크기에서 보여줍니다 (GitHub — deepseek-ai/DeepGEMM: DeepGEMM: clean and efficient FP8 GEMM kernels with fine-grained scaling) (DeepSeek Open Source Week Day 3: DeepGEMM — DEV Community). JIT(Just-In-Time) 컴파일 방식으로 동작하여 런타임에 최적화된 커널을 생성하며, 자체 미세 스케일 보정(fine-grained scaling) 기법과 **2단계 누산(promotion)**을 통해 FP8 연산의 정밀도 손실을 보완합니다 (GitHub — deepseek-ai/DeepGEMM: DeepGEMM: clean and efficient FP8 GEMM kernels with fine-grained scaling) (GitHub — deepseek-ai/DeepGEMM: DeepGEMM: clean and efficient FP8 GEMM kernels with fine-grained scaling). NVIDIA H100(Hopper)에서 1350 TFLOPS 이상의 연산 처리량을 달성하여 V3/R1 모델의 훈련·추론 파이프라인을 가속합니다 (DeepSeek Open Source Week Day 3: DeepGEMM — DEV Community).
